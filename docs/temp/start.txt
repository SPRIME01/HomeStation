# 0) network + foundation (Traefik + MetalLB + cert-manager + Vault + ArgoCD)
just configure-network
just install-foundation

# 1) vault init/unseal/token (non-destructive, idempotent)
just vault-init

# 2) SSO + Redis sessions + clients + Vault K8s onboarding
just sso-bootstrap

# 3) rest of the stack (via Ingress + TLS)
just deploy-core
just deploy-obs
just deploy-ux

# After install, trust the homelab CA on your workstation (optional, for no browser warnings):
# kubectl -n cert-manager get secret homelab-ca -o jsonpath='{.data.ca\.crt}' | base64 -d > homelab-ca.crt
# Then add to OS/browser trust store.


## Next
Link docs index from README: add a “Docs” section pointing to docs/diataxis/index.md.
Validate endpoints: list current Ingress hosts and namespaces for a quick sanity check.
Enable Promtail: PROMTAIL_ENABLED=1 just deploy-obs (if ulimit is ≥ 1048576).
Add CI-friendly docs step: script/just recipe to lint/validate docs links.
Generate a one-pager: update docs/technical-documentation.md to summarize the new docs and key commands.
Publish docs guidance: add quick instructions for GitHub Pages or mkdocs if you want a website.






-----
Got it. Your intent is a homelab with Traefik as the cluster ingress, Vault/ESO for secrets, Ory for SSO, and RabbitMQ as the data fabric. Access should be via Ingress and DNS, not ad-hoc port-forwards. I aligned n8n and RabbitMQ to that model and removed the n8n chart mismatch.

What I changed

n8n install flow: Use the Community Helm Chart without passing your Bitnami values (schema mismatch). We manage Ingress ourselves.
tools/scripts/deploy_core.sh: prefers community-charts/n8n without -f deploy/n8n/values.yaml and still falls back to Bitnami OCI/HTTPS if needed.
Ingresses (Traefik):
RabbitMQ UI at rabbitmq.homelab.lan → deploy/rabbitmq/ingress.yaml
n8n UI at n8n.homelab.lan → deploy/n8n/ingress.yaml
deploy_core.sh applies these after Helm installs.
Why RabbitMQ links “refused to connect”

Those URLs in Helm notes are only valid after you run a port-forward. Since you’re standardizing on cluster ingress, you don’t need them. Use Traefik + DNS instead.
How to proceed (best practice)

Verify Traefik service and IP:
kubectl -n kube-system get svc traefik -o wide
Ensure it’s LoadBalancer and has an external IP from your MetalLB pool.
Add DNS for apps:
Create A-records pointing to Traefik’s external IP:
n8n.homelab.lan
rabbitmq.homelab.lan
Deploy core apps with Ingress:
just deploy-core (or --dry-run first)
Check status: kubectl -n core get deploy,svc,ingress
Open UIs:
n8n: http(s)://n8n.homelab.lan
RabbitMQ: http(s)://rabbitmq.homelab.lan (same credentials as your values; or fetch with the kubectl commands from Helm notes)
About n8n choices

Previous error happened because deploy/n8n/values.yaml is tailored for Bitnami (uses env: and persistence: at root), but you were installing the Community chart which rejects those keys.
Current approach avoids technical debt:
Use the Community chart with defaults (sane for homelab), then a separate, chart-agnostic Ingress manifest.
If/when you want to customize n8n (auth, persistence, image), I can add a deploy/n8n/community-values.yaml compatible with the Community chart and wire it cleanly.
About RabbitMQ as data fabric

You now have an Ingress to the management UI. AMQP stays cluster-internal (rabbitmq.core.svc.cluster.local:5672), which is appropriate.
If you eventually want public AMQP/TLS or mTLS, we can front it with a LoadBalancer or TCP IngressRoute in Traefik, but it’s typically not exposed publicly in homelabs.
Next steps

Want me to:
Add TLS via Traefik with your domain (let’s encrypt or local CA)?
Add a Community n8n values file to enable basic auth/persistence?
Add RabbitMQ Ingress annotations (auth, middlewares) or Kong exposure later?
If anything 404s after DNS is set, share kubectl -n core get ingress,svc,pods and kubectl -n kube-system get svc traefik -o yaml and I’ll drill in.

---

Great — Vault is reachable and healthy; Kubernetes auth just isn’t enabled yet.

Run these locally (with your existing port-forward on 127.0.0.1
):

Export token
source tools/secrets/.envrc.vault
Onboard Vault Kubernetes auth
VAULT_ADDR=http://127.0.0.1:8200 bash tools/scripts/vault_k8s_onboard.sh
The script:
Enables auth/kubernetes
Configures reviewer JWT, host, CA, issuer
Creates policy eso-reader for kv v2 reads under kv/apps/*
Creates role eso bound to SA infra/external-secrets with audiences=api (auto-fallback to bound_audiences for older plugins)
Ensures kv/ mount is v2
Verify immediately:

vault auth list
Expect to see kubernetes/ alongside token/
vault read auth/kubernetes/role/eso
Expect bound_service_account_names=external-secrets, bound_service_account_namespaces=infra, audiences (or bound_audiences)=api
vault secrets list | awk '{print $1}' | grep '^kv/'
Expect kv/
Seed RabbitMQ and deploy:

just vault-seed-kv --only rabbitmq --random
just deploy-core
Readiness checks:

kubectl get clustersecretstore vault-kv -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}'
Expect True
kubectl -n core get secret rabbitmq-auth -o jsonpath='{.data.rabbitmq-password}{" "}{.data.rabbitmq-erlang-cookie}' | awk '{print (length($1)>0 && length($2)>0)? "OK":"MISSING"}'
Expect OK
If anything fails, paste the outputs for:

vault auth list
vault read auth/kubernetes/config
vault read auth/kubernetes/role/eso
kubectl -n infra logs deploy/external-secrets (last 50 lines)
kubectl get clustersecretstore vault-kv -o yaml (conditions)
---
The ClusterSecretStore can’t connect because Vault’s Kubernetes auth isn’t enabled yet. You already have the port-forward up, so let’s onboard Vault and retry.

Do this now

Load token: source tools/secrets/.envrc.vault
Onboard Vault k8s auth:
VAULT_ADDR=http://127.0.0.1:8200 bash tools/scripts/vault_k8s_onboard.sh
Script enables auth/kubernetes, configures reviewer JWT/host/CA/issuer, ensures kv/ v2, and creates role eso bound to infra/external-secrets with audiences api,https://kubernetes.default.svc.cluster.local (handles both token audiences).
Seed RabbitMQ:
just vault-seed-kv --only rabbitmq --random
Re-run deploy:
just deploy-core
Quick checks

vault auth list → shows kubernetes/
vault read auth/kubernetes/role/eso → confirms SA/namespace and audiences
kubectl get clustersecretstore vault-kv -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' → True
kubectl -n core get secret rabbitmq-auth -o jsonpath='{.data.rabbitmq-password} {" "}{.data.rabbitmq-erlang-cookie}' | awk '{print (length($1)&&length($2))?\"OK\":\"MISSING\"}' → OK
If still not ready

Logs: kubectl -n infra logs deploy/external-secrets | tail -n 100
Verify ESO SA: kubectl -n infra get sa external-secrets -o yaml
Verify Vault config:
vault read auth/kubernetes/config
vault read auth/kubernetes/role/eso
Paste any failing outputs and I’ll pinpoint the next tweak.
